
sgd, use Theano for performance. Early stopping.
no significant difference was observed for using relu/sigmoid/tanh
no significant difference was observed for using mse vs cross_entropy

[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 18, 20, 25, 30, 35, 40, 45, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200]

[0.55000000000000004, 0.58000000000000007, 0.87, 0.80499999999999994, 0.84999999999999998, 0.82999999999999996,
0.83499999999999996, 0.87, 0.84499999999999997, 0.79999999999999996, 0.83999999999999997, 0.89500000000000002,
0.89500000000000002, 0.85999999999999999, 0.87, 0.87, 0.87, 0.88500000000000001, 0.85499999999999998,
0.90500000000000003, 0.85999999999999999, 0.85999999999999999, 0.92999999999999994, 0.88500000000000001,
0.91500000000000004, 0.87, 0.91500000000000004, 0.92999999999999994, 0.93500000000000005, 0.89500000000000002,
0.92000000000000004, 0.89000000000000001, 0.92000000000000004, 0.91500000000000004, 0.91000000000000003,
0.90000000000000002]


########lasso###########
magnitude vs nonzeros
lambda: start from 5, decay by 0.9 and look at the number of nonzeros.


# k, train_ac, test_ac
[(1, 0.636666666667, 0.625), (1, 0.636666666667, 0.625), (3, 0.65, 0.635),
(3, 0.65, 0.635), (6, 0.68, 0.64), (9, 0.703333333333, 0.645),
(10, 0.71, 0.65), (11, 0.713333333333, 0.65), (12, 0.723333333333, 0.66),
(16, 0.736666666667, 0.695), (17, 0.75, 0.705), (21, 0.766666666667, 0.735),
(28, 0.85, 0.76), (33, 0.856666666667, 0.765), (36, 0.866666666667, 0.79),
(44, 0.883333333333, 0.815), (51, 0.883333333333, 0.825), (59, 0.913333333333, 0.835),
(66, 0.926666666667, 0.85), (79, 0.936666666667, 0.84), (89, 0.943333333333, 0.855),
(103, 0.96, 0.86), (116, 0.97, 0.855), (133, 0.97, 0.85), (149, 0.973333333333, 0.86),
(165, 0.993333333333, 0.865), (176, 0.99, 0.875), (188, 0.993333333333, 0.875),
(212, 0.953333333333, 0.815), (254, 0.946666666667, 0.795), (284, 0.953333333333, 0.81),
(310, 0.976666666667, 0.85)]

lamb=5.0         num nonzeros in w = 1
[10244]
  train_accuracy=0.636666666667
  test_accuracy=0.625
lamb=4.5         num nonzeros in w = 1
[10244]
  train_accuracy=0.636666666667
  test_accuracy=0.625
lamb=4.05        num nonzeros in w = 3
[626, 10244, 12610]
  train_accuracy=0.65
  test_accuracy=0.635
lamb=3.645       num nonzeros in w = 3
[626, 10244, 12610]
  train_accuracy=0.65
  test_accuracy=0.635
lamb=3.2805      num nonzeros in w = 6
[626, 1565, 7709, 9614, 10244, 12610]
  train_accuracy=0.68
  test_accuracy=0.64
lamb=2.95245     num nonzeros in w = 9
[626, 1565, 4308, 6866, 9614, 10244, 12610, 13685, 15798]
  train_accuracy=0.703333333333
  test_accuracy=0.645
lamb=2.657205    num nonzeros in w = 10
[626, 1565, 4308, 6866, 9614, 10244, 12170, 12610, 13685, 15798]
  train_accuracy=0.71
  test_accuracy=0.65
lamb=2.3914845   num nonzeros in w = 11
[626, 1565, 4308, 6866, 9614, 10244, 10779, 12170, 12610, 13685, 15798]
  train_accuracy=0.713333333333
  test_accuracy=0.65
lamb=2.15233605  num nonzeros in w = 12
[626, 1565, 4308, 6866, 8786, 9614, 10244, 10779, 12170, 12610, 13685, 15798]
  train_accuracy=0.723333333333
  test_accuracy=0.66
lamb=1.937102445         num nonzeros in w = 16
[626, 1565, 3433, 4308, 5128, 6866, 7709, 8786, 9614, 10244, 10779, 12170, 12610, 13685, 15798, 19386]
  train_accuracy=0.736666666667
  test_accuracy=0.695
lamb=1.7433922005        num nonzeros in w = 17
[626, 1565, 3433, 4308, 5128, 6866, 7709, 8786, 9614, 10244, 10779, 12170, 12610, 13685, 15798, 19327, 19386]
  train_accuracy=0.75
  test_accuracy=0.705
lamb=1.56905298045       num nonzeros in w = 21
[268, 626, 1565, 3433, 4308, 4637, 5128, 6866, 7709, 8786, 9614, 10244, 10532, 10779, 12170, 12610, 13165, 13685, 15798, 19327]
  train_accuracy=0.766666666667
  test_accuracy=0.735
lamb=1.41214768241       num nonzeros in w = 28
[268, 626, 1565, 3433, 4308, 4637, 4722, 5128, 6866, 7709, 8786, 8789, 9614, 10244, 10532, 10779, 11668, 12170, 12610, 13165]
  train_accuracy=0.85
  test_accuracy=0.76
lamb=1.27093291416       num nonzeros in w = 33
[268, 626, 1565, 3433, 4153, 4308, 4637, 4722, 5128, 6865, 6866, 7709, 8786, 8789, 9614, 10244, 10457, 10532, 10779, 11668]
  train_accuracy=0.856666666667
  test_accuracy=0.765
lamb=1.14383962275       num nonzeros in w = 36
[268, 626, 1052, 1244, 1565, 3433, 4153, 4191, 4308, 4637, 4722, 5128, 6865, 6866, 7709, 8786, 8789, 9614, 10244, 10457]
  train_accuracy=0.866666666667
  test_accuracy=0.79
lamb=1.02945566047       num nonzeros in w = 44
[268, 626, 1052, 1244, 1565, 2612, 3433, 4153, 4191, 4308, 4637, 4722, 5128, 5507, 6084, 6865, 6866, 7709, 8786, 8789]
  train_accuracy=0.883333333333
  test_accuracy=0.815
lamb=0.926510094426      num nonzeros in w = 51
[268, 626, 1052, 1244, 1565, 2612, 2700, 3433, 4153, 4191, 4308, 4494, 4614, 4637, 4722, 5128, 5507, 6084, 6865, 6866]
  train_accuracy=0.883333333333
  test_accuracy=0.825
lamb=0.833859084983      num nonzeros in w = 59
[268, 626, 1052, 1244, 1565, 2612, 2700, 3433, 3734, 4153, 4191, 4308, 4494, 4614, 4637, 4722, 5128, 5507, 6084, 6865]
  train_accuracy=0.913333333333
  test_accuracy=0.835
lamb=0.750473176485      num nonzeros in w = 66
[268, 626, 1052, 1244, 1565, 2612, 2700, 2934, 3433, 3515, 3734, 4153, 4191, 4308, 4494, 4614, 4637, 4722, 5128, 5353]
  train_accuracy=0.926666666667
  test_accuracy=0.85
lamb=0.675425858836      num nonzeros in w = 79
[218, 268, 626, 1052, 1244, 1565, 2612, 2700, 2934, 3433, 3734, 4153, 4191, 4308, 4494, 4614, 4637, 4722, 4857, 5016]
  train_accuracy=0.936666666667
  test_accuracy=0.84
lamb=0.607883272953      num nonzeros in w = 89
[218, 268, 626, 1052, 1244, 1565, 2612, 2700, 2934, 3433, 3734, 4153, 4191, 4308, 4347, 4494, 4614, 4637, 4722, 4857]
  train_accuracy=0.943333333333
  test_accuracy=0.855
lamb=0.547094945658      num nonzeros in w = 103
[218, 268, 626, 732, 1052, 1244, 1565, 2612, 2700, 2934, 3433, 3709, 3734, 3826, 4153, 4191, 4308, 4347, 4494, 4614]
  train_accuracy=0.96
  test_accuracy=0.86
lamb=0.492385451092      num nonzeros in w = 116
[104, 218, 268, 476, 626, 732, 989, 1052, 1057, 1244, 1525, 1565, 2612, 2700, 2934, 3433, 3709, 3734, 3826, 4153]
  train_accuracy=0.97
  test_accuracy=0.855
lamb=0.443146905983      num nonzeros in w = 133
[104, 218, 268, 476, 626, 732, 989, 1052, 1057, 1145, 1244, 1525, 1565, 2612, 2700, 2828, 2934, 3433, 3709, 3734]
  train_accuracy=0.97
  test_accuracy=0.85
lamb=0.398832215384      num nonzeros in w = 149
[104, 218, 268, 476, 626, 732, 989, 1052, 1057, 1145, 1244, 1525, 1565, 2612, 2700, 2828, 2934, 3119, 3433, 3709]
  train_accuracy=0.973333333333
  test_accuracy=0.86
lamb=0.358948993846      num nonzeros in w = 165
[104, 218, 268, 476, 626, 732, 989, 1052, 1057, 1145, 1244, 1525, 1565, 1798, 2185, 2612, 2700, 2751, 2777, 2828]
  train_accuracy=0.993333333333
  test_accuracy=0.865
lamb=0.323054094461      num nonzeros in w = 176
[104, 218, 268, 443, 476, 626, 732, 737, 989, 1052, 1057, 1145, 1244, 1525, 1565, 1798, 2185, 2612, 2700, 2751]
  train_accuracy=0.99
  test_accuracy=0.875
lamb=0.290748685015      num nonzeros in w = 188
[104, 218, 268, 443, 476, 626, 732, 737, 989, 1052, 1057, 1145, 1244, 1525, 1565, 1798, 2071, 2185, 2612, 2700]
  train_accuracy=0.993333333333
  test_accuracy=0.875
lamb=0.261673816514      num nonzeros in w = 212
[104, 151, 370, 626, 1040, 1052, 1525, 1565, 1598, 1798, 1914, 1968, 2015, 2062, 2185, 2210, 2700, 2707, 2840, 3119]
  train_accuracy=0.953333333333
  test_accuracy=0.815
lamb=0.235506434862      num nonzeros in w = 254
[104, 120, 151, 247, 343, 476, 496, 626, 1040, 1145, 1173, 1316, 1525, 1538, 1565, 1598, 1798, 1914, 1968, 2015]
  train_accuracy=0.946666666667
  test_accuracy=0.795
lamb=0.211955791376      num nonzeros in w = 284
[87, 106, 120, 151, 218, 247, 343, 476, 496, 499, 626, 664, 1040, 1052, 1057, 1145, 1173, 1214, 1316, 1525]
  train_accuracy=0.953333333333
  test_accuracy=0.81
lamb=0.190760212238      num nonzeros in w = 310
[87, 106, 120, 151, 218, 247, 285, 343, 476, 496, 499, 626, 1052, 1057, 1214, 1268, 1316, 1525, 1565, 1598]
  train_accuracy=0.976666666667
  test_accuracy=0.85






#########SFG#########

top features:
    features_added = [12916, 12610, 3433, 17104, 19674, 14844, 4637, 3870, 7577,
        18201, 2851, 17356, 5435, 18224, 9734, 16511, 5801, 4074, 3147, 15937,
        7651, 2982, 14994, 9419, 16509, 4325, 15629, 14299, 5075, 13587, 12552,
        4644, 17228, 14645, 3313, 17812, 8361, 3814, 12233, 11324, 5575, 18976,
        2472, 12042, 5658, 7652, 14573, 14892, 18801, 11188, 12026, 12245, 9494,
        17541, 17700, 5549, 12311, 19863, 13398, 16561, 1058, 2368, 619, 10539,
        17484, 5203, 17080, 19109, 10292, 4857]

k: 1..70
test_ac =
[0.70999999999999996, 0.80000000000000004, 0.82499999999999996, 0.82499999999999996, 0.82999999999999996,
0.83999999999999997, 0.84499999999999997, 0.84499999999999997, 0.84499999999999997, 0.84499999999999997,
0.84499999999999997, 0.84999999999999998, 0.84499999999999997, 0.84999999999999998, 0.84999999999999998,
0.84499999999999997, 0.84499999999999997, 0.84499999999999997, 0.84499999999999997, 0.84499999999999997,
0.84499999999999997, 0.84499999999999997, 0.84499999999999997, 0.84499999999999997, 0.84499999999999997,
0.84499999999999997, 0.84499999999999997, 0.84499999999999997, 0.84499999999999997, 0.84499999999999997,
0.84499999999999997, 0.84499999999999997, 0.84499999999999997, 0.84499999999999997, 0.84499999999999997,
0.84499999999999997, 0.84999999999999998, 0.84999999999999998, 0.84999999999999998, 0.84999999999999998,
0.84999999999999998, 0.84999999999999998, 0.84999999999999998, 0.84999999999999998, 0.84999999999999998,
0.84999999999999998, 0.84999999999999998, 0.84999999999999998, 0.84999999999999998, 0.84999999999999998,
0.84999999999999998, 0.84999999999999998, 0.84999999999999998, 0.84999999999999998, 0.84999999999999998,
0.84999999999999998, 0.84999999999999998, 0.84999999999999998, 0.84999999999999998, 0.84999999999999998,
0.84999999999999998, 0.84999999999999998, 0.84999999999999998, 0.84999999999999998, 0.84999999999999998,
0.84999999999999998, 0.84999999999999998, 0.84999999999999998, 0.84999999999999998, 0.84999999999999998]