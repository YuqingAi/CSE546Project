import numpy as np
import scipy.io as io
import scipy.sparse as sp
import math
from sklearn.svm import SVC
import matplotlib.pyplot as plt

import lasso

def load_data(X_filename):
    d = 20000
    res = []
    with open(X_filename) as f:
        for line in f:
            cur = np.zeros((d,))
            for token in line.split():
                if len(token) > 1:
                    [feature, val] = token.split(":")
                    cur[int(feature)] = int(val) / 1000.0
            res += [cur]
    return np.array(res)

def load_labels(Y_filename):
    res = []
    with open(Y_filename) as f:
        for line in f:
            if len(line) > 1:
                res += [1 if int(line) > 0 else 0]
    return np.array(res)

def lasso_predict(w_hat, w_0_hat, X):
    res = X.dot(w_hat) + w_0_hat
    return np.array([1 if res[i] > 0.5 else 0 for i in range(res.shape[0])])

def loss01(Y_predict, Y_real):
    return np.sum(Y_predict != Y_real)

def accuracy(Y_predict, Y_real):
    return 1 - loss01(Y_predict, Y_real) * 1.0 / Y_real.shape[0]

def solve():
    X_train = sp.csc_matrix(load_data("dexter_train.data"))
    Y_train = load_labels("dexter_train.labels")
    X_valid = sp.csc_matrix(load_data("dexter_valid.data"))
    Y_valid = load_labels("dexter_valid.labels")
    print "data loaded"
    # w_hat, w_0, best_lamb = lasso.solve_lasso(X_train, Y_train, X_valid, Y_valid)    
    # best_lamb = 0.123
    # w_hat, w_0_hat = lasso.solve_lasso_with_lamb(X_train, Y_train, best_lamb)

    w_hat = np.zeros((20000,))
    w_hat[104] = 0.382631856716
    w_hat[106] = -0.537504551671
    w_hat[120] = 1.2796994543
    w_hat[151] = -1.04476653475
    w_hat[218] = -0.233564647845
    w_hat[225] = -1.32687484943
    w_hat[247] = -1.89383093173
    w_hat[285] = 0.273844057278
    w_hat[343] = -3.32912844407
    w_hat[476] = -0.799478512473
    w_hat[496] = 2.57674414151
    w_hat[499] = 0.709774942012
    w_hat[626] = -3.09556474571
    w_hat[664] = 0.166412958309
    w_hat[981] = 0.524981298931
    w_hat[1022] = 0.0430794297545
    w_hat[1052] = 1.37950434139
    w_hat[1057] = 1.4095076819
    w_hat[1145] = 0.222674661304
    w_hat[1214] = -0.645937192315
    w_hat[1268] = 1.82095379552
    w_hat[1316] = 2.06151289102
    w_hat[1462] = -0.29605570011
    w_hat[1525] = 0.817423118618
    w_hat[1565] = -0.559041270774
    w_hat[1598] = 2.18158686055
    w_hat[1599] = 0.0662175834725
    w_hat[1798] = 4.89847232047
    w_hat[1914] = -0.960082489326
    w_hat[1953] = 2.76129446671
    w_hat[1968] = -0.792400597548
    w_hat[2000] = 0.556337493583
    w_hat[2015] = -0.997349468043
    w_hat[2062] = -0.27237306676
    w_hat[2071] = 1.15922876457
    w_hat[2133] = -1.40533669082
    w_hat[2210] = 2.34831292591
    w_hat[2339] = -1.99542257169
    w_hat[2532] = -0.222320027013
    w_hat[2659] = -1.71682095678
    w_hat[2700] = 2.53768747863
    w_hat[2728] = 0.0027673608943
    w_hat[2751] = -0.815817001481
    w_hat[2820] = -2.155282894
    w_hat[2828] = -0.177438537329
    w_hat[2840] = -1.31158020818
    w_hat[2915] = -0.354553239548
    w_hat[2934] = 0.127677187777
    w_hat[2950] = -0.444271549372
    w_hat[2983] = -1.40708834141
    w_hat[2990] = 1.32936502472
    w_hat[3084] = 0.55564105591
    w_hat[3119] = 0.0376082885629
    w_hat[3150] = -1.43588254837
    w_hat[3221] = 1.16461249514
    w_hat[3265] = -1.45551422997
    w_hat[3355] = -0.963243599997
    w_hat[3470] = 1.76192056044
    w_hat[3515] = -0.493366034054
    w_hat[3616] = 1.69945680623
    w_hat[3617] = -5.28587042342
    w_hat[3664] = -0.257708597893
    w_hat[3709] = -0.446548558042
    w_hat[3908] = 2.0215908683
    w_hat[3924] = -0.167338471887
    w_hat[4132] = 4.61490603672
    w_hat[4308] = 1.82018862231
    w_hat[4347] = 0.163211483407
    w_hat[4376] = -0.00775898056292
    w_hat[4392] = 0.59255403821
    w_hat[4424] = 2.62567511747
    w_hat[4500] = -1.58441035209
    w_hat[4541] = 0.529146900208
    w_hat[4590] = -2.78648869917
    w_hat[4614] = 2.29717885254
    w_hat[4619] = 1.86942196778
    w_hat[4698] = -0.0838230501306
    w_hat[4722] = 1.82273468287
    w_hat[4857] = -0.0840554833691
    w_hat[4896] = 1.08327503563
    w_hat[4957] = 4.08157848475
    w_hat[5041] = 0.944102858147
    w_hat[5047] = 0.304726171471
    w_hat[5128] = 1.23720559558
    w_hat[5143] = 0.311934649223
    w_hat[5241] = -0.949132062073
    w_hat[5305] = 0.0525202647053
    w_hat[5368] = 0.120684970145
    w_hat[5382] = -0.0940095872722
    w_hat[5383] = -0.250737883788
    w_hat[5417] = -1.05762891296
    w_hat[5660] = -0.771404875307
    w_hat[5727] = 0.856253865001
    w_hat[5730] = 0.0785384723661
    w_hat[5838] = -0.00812745757056
    w_hat[5874] = -2.03992459934
    w_hat[5877] = 2.11058054456
    w_hat[5933] = -0.294122423665
    w_hat[6007] = -2.18666395936
    w_hat[6018] = 3.05231358857
    w_hat[6024] = 0.607686356407
    w_hat[6234] = 2.01665538042
    w_hat[6324] = -3.59087688425
    w_hat[6372] = 0.863600557997
    w_hat[6452] = 0.769261197085
    w_hat[6502] = -1.48051712182
    w_hat[6577] = 2.24340344542
    w_hat[6636] = 1.67331905959
    w_hat[6662] = 2.14274165476
    w_hat[6714] = -2.05619848309
    w_hat[6774] = 2.50251505996
    w_hat[6850] = 4.50912986245
    w_hat[6865] = 1.50028160754
    w_hat[6866] = 4.47574534725
    w_hat[6927] = 0.224498326894
    w_hat[7002] = 0.00595788791741
    w_hat[7087] = 3.0172733745
    w_hat[7174] = 0.325351453592
    w_hat[7182] = 0.957822880717
    w_hat[7183] = 1.64625964951
    w_hat[7248] = 1.98133384739e-05
    w_hat[7363] = 0.292712718784
    w_hat[7371] = 2.22533755678
    w_hat[7407] = 4.84344540929
    w_hat[7443] = -0.860091633871
    w_hat[7445] = 0.733656290853
    w_hat[7494] = 3.20341183144
    w_hat[7539] = 2.38333040397
    w_hat[7596] = -0.0826502951711
    w_hat[7625] = 5.45236342902
    w_hat[7709] = 3.2055229191
    w_hat[7908] = 0.621153336856
    w_hat[7916] = -1.24875481518
    w_hat[8074] = -0.728032119598
    w_hat[8148] = 1.99868541038
    w_hat[8202] = 1.53574559335
    w_hat[8241] = 1.99153534126
    w_hat[8261] = 0.00485203301616
    w_hat[8288] = 4.6731168025
    w_hat[8475] = 0.474245322607
    w_hat[8480] = 0.9406464101
    w_hat[8488] = 0.572006260521
    w_hat[8703] = 0.591739609667
    w_hat[8786] = -2.25509021439
    w_hat[8789] = 4.40709841995
    w_hat[8792] = -0.951498719846
    w_hat[8804] = 0.384454458108
    w_hat[8813] = 0.717626828057
    w_hat[8895] = 1.10445540766
    w_hat[8920] = 1.82357499054
    w_hat[8986] = 5.24809387943
    w_hat[8996] = 1.62785658504
    w_hat[9075] = 3.04419022297
    w_hat[9109] = 0.0321901001535
    w_hat[9180] = 2.91329415367
    w_hat[9249] = 0.263437723186
    w_hat[9251] = -1.80650757439
    w_hat[9325] = 0.0159074130779
    w_hat[9390] = 1.00633412545
    w_hat[9781] = -2.08749063112
    w_hat[9782] = 2.02318637885
    w_hat[9849] = -0.0537045358841
    w_hat[9856] = -2.47826940934
    w_hat[9860] = 0.820392422874
    w_hat[9867] = 0.544413467973
    w_hat[9976] = 0.50998557465
    w_hat[10043] = 2.76198060153
    w_hat[10154] = -0.277796216708
    w_hat[10161] = 3.07724306373
    w_hat[10187] = 2.01030445593
    w_hat[10218] = -0.174970137315
    w_hat[10241] = 3.13801147195
    w_hat[10244] = -0.206864925312
    w_hat[10408] = 0.0780204399157
    w_hat[10419] = 2.22557256319
    w_hat[10443] = 3.61272965436
    w_hat[10532] = 3.9398392495
    w_hat[10565] = 0.486490404903
    w_hat[10626] = 2.46251751138
    w_hat[10632] = 0.125747398704
    w_hat[10663] = -1.66994864569
    w_hat[10703] = 1.41334762782
    w_hat[10744] = 0.683623861716
    w_hat[10779] = 0.314614623828
    w_hat[10814] = 0.620751029363
    w_hat[10833] = 1.870875642
    w_hat[10848] = 2.29400904384
    w_hat[10939] = -3.46029631909
    w_hat[11120] = -1.54295623624
    w_hat[11245] = 0.0302262530503
    w_hat[11303] = 0.138081219131
    w_hat[11310] = 0.99688193167
    w_hat[11380] = -0.490848993915
    w_hat[11395] = 2.41461479139
    w_hat[11400] = 2.82589694967
    w_hat[11440] = 2.2409236203
    w_hat[11447] = 0.0348576422262
    w_hat[11460] = -0.225628712291
    w_hat[11588] = -1.55290779365
    w_hat[11642] = 1.60253417386
    w_hat[11668] = 2.39670000613
    w_hat[11685] = 0.432485336173
    w_hat[11695] = 2.12993740833
    w_hat[11822] = 0.926120182091
    w_hat[11871] = 2.16571891696
    w_hat[11899] = -0.60464352029
    w_hat[11966] = -1.60522602233
    w_hat[11999] = 0.565413288998
    w_hat[12007] = 1.42225443916
    w_hat[12094] = 0.3967471232
    w_hat[12113] = 0.768443780085
    w_hat[12127] = -0.189173241486
    w_hat[12136] = 1.46332073034
    w_hat[12156] = 1.62820953296
    w_hat[12170] = -0.215178173577
    w_hat[12217] = 1.85824914373
    w_hat[12238] = 4.2056699916
    w_hat[12320] = 1.04964848939
    w_hat[12380] = 1.49766592927
    w_hat[12406] = 0.272136212489
    w_hat[12412] = 0.954528667433
    w_hat[12433] = 2.74695551454
    w_hat[12435] = 2.81660039829
    w_hat[12438] = 2.80934008749
    w_hat[12448] = 0.259753468564
    w_hat[12466] = 4.05220097229
    w_hat[12595] = 0.202593359943
    w_hat[12599] = 2.07156412527
    w_hat[12604] = 2.76408697349
    w_hat[12607] = 1.5454829068
    w_hat[12610] = 2.66264696543
    w_hat[12674] = 1.73597918452
    w_hat[12712] = -0.667456224912
    w_hat[12798] = 5.48812196939
    w_hat[12893] = 3.11852034163
    w_hat[12968] = 5.46069418823
    w_hat[12992] = -2.18201146437
    w_hat[12994] = 0.303277203705
    w_hat[13098] = 1.03537784891
    w_hat[13143] = 5.65053669074
    w_hat[13190] = 0.00927933708749
    w_hat[13372] = -0.75476402616
    w_hat[13378] = 7.02842855954
    w_hat[13384] = 3.66629508187
    w_hat[13468] = 1.40940407929
    w_hat[13542] = 0.250020952744
    w_hat[13565] = 3.20811243008
    w_hat[13607] = 0.0726909410038
    w_hat[13685] = 3.23650378084
    w_hat[13691] = 2.0086677187
    w_hat[13759] = 0.153701138798
    w_hat[13829] = 4.57616390339
    w_hat[13881] = 2.2291448255
    w_hat[13929] = 5.6962580322
    w_hat[14087] = 2.61770496873
    w_hat[14133] = 0.0994893313833
    w_hat[14161] = 6.04990520383
    w_hat[14245] = 3.57252270656
    w_hat[14278] = 1.43543725717
    w_hat[14513] = 0.403345410105
    w_hat[14529] = -0.977472995187
    w_hat[14593] = -1.40003838023
    w_hat[14662] = -2.12461125227
    w_hat[14729] = 3.85401570374
    w_hat[14851] = 0.044261816705
    w_hat[14872] = 0.552922234831
    w_hat[15277] = 4.85174056381
    w_hat[15294] = 2.80663792309
    w_hat[15344] = 0.293958980274
    w_hat[15362] = -2.60539547384
    w_hat[15448] = 4.18700826772
    w_hat[15450] = 0.707273380972
    w_hat[15492] = 4.63212618138
    w_hat[15523] = 6.37873439276
    w_hat[15558] = 0.615859503263
    w_hat[15619] = 0.486689531745
    w_hat[15640] = 3.13125394033
    w_hat[15798] = 4.58238338184
    w_hat[15806] = 2.50401128364
    w_hat[15859] = -0.37309927057
    w_hat[15868] = 3.95145093933
    w_hat[15948] = -0.0728799418558
    w_hat[16022] = 2.58543539961
    w_hat[16029] = 1.19892576562
    w_hat[16034] = 0.236941045797
    w_hat[16083] = 2.55887352277
    w_hat[16130] = -0.0149465823035
    w_hat[16186] = 2.28611659216
    w_hat[16215] = -1.37395289652
    w_hat[16218] = 1.29858674512
    w_hat[16232] = 0.708986779803
    w_hat[16288] = 0.0225021866411
    w_hat[16325] = 4.07327688944
    w_hat[16339] = 0.362410769284
    w_hat[16367] = 1.0670182372
    w_hat[16478] = 6.18702659666
    w_hat[16584] = 0.95636322641
    w_hat[16651] = 0.449658069805
    w_hat[16721] = 0.626033608457
    w_hat[16810] = 2.65212384823
    w_hat[17128] = 2.76186889959
    w_hat[17237] = 0.728810251021
    w_hat[17365] = 4.10718088418
    w_hat[17430] = -0.986740020809
    w_hat[17487] = 1.06654208924
    w_hat[17526] = 0.0329777517183
    w_hat[17546] = -2.51681663441
    w_hat[17613] = 0.669553634579
    w_hat[17632] = 0.612496631003
    w_hat[17753] = 0.108079090617
    w_hat[17799] = 2.6098226279
    w_hat[17841] = 5.49030215364
    w_hat[17876] = -0.472406647368
    w_hat[17896] = 2.01070801958
    w_hat[17942] = 0.0916661047107
    w_hat[18115] = 8.29280221515
    w_hat[18364] = 0.788311952071
    w_hat[18406] = 2.20684108965
    w_hat[18413] = 3.09854344846
    w_hat[18416] = -0.896492520645
    w_hat[18559] = 2.89611907011
    w_hat[18711] = -1.41166500337
    w_hat[18742] = 4.4005791084
    w_hat[18800] = 3.8426018475
    w_hat[18850] = 0.179140109401
    w_hat[18916] = 4.9250986026
    w_hat[18959] = 2.41287498028
    w_hat[18964] = 0.876989138075
    w_hat[19024] = 5.10811597468
    w_hat[19109] = 5.1665970881
    w_hat[19160] = -0.97326224689
    w_hat[19247] = 3.98634607033
    w_hat[19255] = 2.64831730072
    w_hat[19287] = 0.182214564314
    w_hat[19309] = 0.967123673414
    w_hat[19327] = 6.15068880413
    w_hat[19386] = 11.3080074917
    w_hat[19430] = 2.08927188029
    w_hat[19473] = 0.131947736046
    w_hat[19516] = 2.00942123878
    w_hat[19685] = 5.10205094144
    w_hat[19694] = -0.657879829068
    w_hat[19707] = 0.917777326737
    w_hat[19774] = 0.443569430051
    w_hat[19776] = 2.64910555282
    w_hat[19787] = 4.42287571585
    w_hat[19823] = 0.11112319389
    w_hat[19890] = 0.202052558075
    w_hat[19908] = -2.34294583624
    w_hat[19921] = 8.58372568202
    w_hat[19926] = 1.12708481618
    w_hat[19976] = 3.6060480767

    w_0_hat = -6.26287299149

    print "num nonzeros in w = " + str(lasso.num_nonzeros(w_hat))

    nonzeros = []
    for i in range(len(w_hat)):
        if (abs(w_hat[i]) > 1e-09):
            nonzeros += [i]
    print nonzeros

    sorted_features = np.argsort(np.absolute(w_hat))[::-1]
    sorted_features = sorted_features[0:300]
    print sorted_features

    accuracy2fold = []
    selected = []
    n, d = X_train.shape
    m, d = X_valid.shape

    for next_best in sorted_features:
        selected += [next_best]
        X_train_selected = X_train[:, selected]
        X_valid_selected = X_valid[:, selected]

        clf = SVC(kernel="linear")
        clf.fit(X_train_selected, Y_train)
        prediction = clf.predict(X_valid_selected)
        accuracy = 1 - loss01(prediction, Y_valid) * 1.0 / m

        clf = SVC(kernel="linear")
        clf.fit(X_valid_selected, Y_valid)
        prediction = clf.predict(X_train_selected)
        accuracy += 1 - loss01(prediction, Y_train) * 1.0 / n
        accuracy /= 2

        print accuracy
        accuracy2fold += [accuracy]

    xs = np.array(range(len(accuracy2fold))) + 1

    plt.plot(xs[2:20], accuracy2fold[2:20], '-r')
    plt.ylabel('Accuracy')
    plt.xlabel('k')

    plt.figure()
    plt.plot(xs[10:300], accuracy2fold[10:300], '-r')
    plt.ylabel('Accuracy')
    plt.xlabel('k')

    plt.show()


if __name__ == "__main__":
    solve()

